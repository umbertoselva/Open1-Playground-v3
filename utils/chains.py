# set the model
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain

def chain(model_name: str, 
          temperature: float,
          QA_PROMPT: str, 
          CONDENSE_QUESTION_PROMPT: str,
          chat_history: list, 
          question: str,
          docs: list, 
          metadata: list) -> tuple[str, str]:
    """
    Retrieval QA chain with memory

    Args:
        model_name (str): LLM model e.g. 'gpt-3.5-turbo-16k'
        temperature (float): model temperature, e.g 0.0
        QA_PROMPT (str): the prompt that generates the answer
        CONDENSE_QUESTION_PROMPT: the prompt that condenses a follow-up question if related to chat history
        chat_history (list): List of previous QA exchanges (st.session_state['history'])
        question (str): the user question
        docs (list): list of str (the retrieved chunks passed to the prompt)
        metadata (list): list of the metadata (chapter or page number) corresponding to each chunk
    Returns:
        output (str): the response generated by the LLM
        condensed_question (str): the question generated in an intermediate LLM call to handle memory + retrieval
    """

    # set model
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature
    )

    # set chains
    qa_chain = LLMChain(
            llm=llm,
            prompt=QA_PROMPT
        )
    
    condense_chain = LLMChain(
            llm=llm,
            prompt=CONDENSE_QUESTION_PROMPT
        )                    

    # if there is no history go straight to running QA chain
    if chat_history == []:

        condensed_question = ""

        output = qa_chain.run(
            question=question,
            doc01=docs[0],
            doc02=docs[1],
            doc03=docs[2],
            doc04=docs[3],
            metadata01=metadata[0],
            metadata02=metadata[1],
            metadata03=metadata[2],
            metadata04=metadata[3],
        )

    # if there is history, condense user question first
    else:

        # condense user input
        condensed_question = condense_chain.run(
            chat_history=chat_history,
            question=question,
        )

        # Debug condensed question
        print("Condensed question:", condensed_question)

        # then run the QA chain
        output = qa_chain.run(
            question=condensed_question,
            doc01=docs[0],
            doc02=docs[1],
            doc03=docs[2],
            doc04=docs[3],
            metadata01=metadata[0],
            metadata02=metadata[1],
            metadata03=metadata[2],
            metadata04=metadata[3],
        )
    
    return output, condensed_question